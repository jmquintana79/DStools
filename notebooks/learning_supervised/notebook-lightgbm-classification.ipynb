{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fd08a5c6-a8fb-4bd2-a358-5ba886c151dc",
   "metadata": {},
   "source": [
    "# Supervised Learning with Light Gradient Boosting - Classification\n",
    "\n",
    "Lo que pretendo es sentar las bases para un algoritmo ganador que valga para todo de tal manera que pueda centrar mas esfuerzos en:\n",
    "- features engineering\n",
    "- interpretability (SHAP)\n",
    "- model evaluation (simple train-test is not enough)\n",
    "\n",
    "El modelo escogido es Light Gradient Boosting por las siguientes razones:\n",
    "- Gradient Boosting suele tener un **buen desempeño** en multiples tipos de problemas. En Kaggle el ranking de modelos ganadores es el siguiente: 1-Keras, 2-LightGBM, 3-GBoost.\n",
    "- Al estar basado en arboles de decision:\n",
    "    - **Es inmune a los missing values** por lo que no hay que preocuparse por su imputación (sólo tener en cuenta de NO admitir missing values en test si no los hay en el training).\n",
    "    - **Simple categorical encoding**: Las variables categoricas pueden ser codificadas como ordinales. El one-hot-encoding no se suele sentar muy bien ante una elevada cardinalidad.\n",
    "- La libreria lightgbm admite ademas features muy interesantes:\n",
    "    - Pesado de label ante desbalanceo (no necesario under/over-fitting).\n",
    "    - Posible seleccionar en multiples loss functions segun tipo de problema. Tb admite custom. \n",
    "    - Hiperparametros mas interesantes: num de arboles, learning rate. Los demas son para evitar el over-fitting de los propios arboles y los valores por defecto son suficientes. NOTA: no usar random-search para optimizacion de hiperparametros.\n",
    "    - Admite Spark.\n",
    "    - Monotone constrains.\n",
    "- En el caso de que la mayoria de los features sean categoricos, puede ser usado CatBoost.\n",
    "\n",
    "#### References:\n",
    "- [GitHub - Light Gradient Boosting Machine](https://github.com/microsoft/LightGBM)\n",
    "- [lightgbm - ReadDocs](https://lightgbm.readthedocs.io/en/latest/index.html)\n",
    "- [MachineLearningMasgtery - Gradient Boosting with Scikit-Learn, XGBoost, LightGBM, and CatBoost\n",
    "](https://machinelearningmastery.com/gradient-boosting-with-scikit-learn-xgboost-lightgbm-and-catboost/)\n",
    "- [Paper - LightGBM: A Highly Efficient Gradient Boosting Decision Tree](https://papers.nips.cc/paper/2017/hash/6449f44a102fde848669bdd9eb6b76fa-Abstract.html) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0207ec70-10da-413d-a1dd-e2aa54ee6107",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip freeze > requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d7dd12ef-5ab5-4224-9252-7d226196510d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.2.1\n"
     ]
    }
   ],
   "source": [
    "# check lightgbm version\n",
    "import lightgbm\n",
    "print(lightgbm.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53b60aab-ba31-4e62-9390-12f58ad6421e",
   "metadata": {},
   "source": [
    "### Test LightGBM Sklearn API (example by MachineLearningMastery)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "10cd5cb2-787c-4a77-b472-95a169cf84a9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1: 0.934 (0.022)\n",
      "Prediction: 1\n"
     ]
    }
   ],
   "source": [
    "# lightgbm for classification\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from sklearn.datasets import make_classification\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import RepeatedStratifiedKFold\n",
    "from matplotlib import pyplot\n",
    "# define dataset\n",
    "X, y = make_classification(n_samples=1000, n_features=10, n_informative=5, n_redundant=5, random_state=1)\n",
    "# evaluate the model\n",
    "model = LGBMClassifier()\n",
    "cv = RepeatedStratifiedKFold(n_splits=10, n_repeats=3, random_state=1)\n",
    "n_scores = cross_val_score(model, X, y, scoring='f1', cv=cv, n_jobs=-1, error_score='raise')\n",
    "print('F1: %.3f (%.3f)' % (mean(n_scores), std(n_scores)))\n",
    "# fit the model on the whole dataset\n",
    "clf = LGBMClassifier()\n",
    "clf.fit(X, y)\n",
    "# make a single prediction\n",
    "row = [[2.56999479, -0.13019997, 3.16075093, -4.35936352, -1.61271951, -1.39352057, -2.48924933, -1.93094078, 3.26130366, 2.05692145]]\n",
    "yhat = clf.predict(row)\n",
    "print('Prediction: %d' % yhat[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63219272-a7e7-4b7c-9032-4a7bcc715ce8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627c7f64-d59a-474a-9374-79974f8c0f7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0b5a28c-eba8-42ca-9fe5-61270defd1cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "prediction",
   "language": "python",
   "name": "prediction"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
