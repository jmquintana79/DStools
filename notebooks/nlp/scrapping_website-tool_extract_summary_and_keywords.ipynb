{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fc850ef2",
   "metadata": {},
   "source": [
    "# scrapping website: tool_extract_summary_and_keywords"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3749b5ec",
   "metadata": {},
   "source": [
    "!pip3 install readability\n",
    "!pip3 install readability-lxml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f47cb85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from newspaper import Article\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from readability.readability import Document as Paper\n",
    "from requests.packages.urllib3.exceptions import InsecureRequestWarning\n",
    "import re\n",
    "import platform\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f21f3f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part of code sourced from https://gist.github.com/linwoodc3/e12a7fbebfa755e897697165875f8fdb\n",
    "\n",
    "def get_url_text(url):\n",
    "\n",
    "    done = {}\n",
    "\n",
    "    TAGS = ['h1', 'h2', 'h3', 'h4', 'h5', 'h6', 'h7', 'p', 'li']\n",
    "\n",
    "    # regex for url check\n",
    "    s = re.compile('(http://|https://)([A-Za-z0-9_\\.-]+)')\n",
    "    u = re.compile(\"(http://|https://)(www.)?(.*)(\\.[A-Za-z0-9]{1,4})$\")\n",
    "    if s.search(url):\n",
    "        site = u.search(s.search(url).group()).group(3)\n",
    "    else:\n",
    "        site = None\n",
    "    answer = {}\n",
    "    # check that its an url\n",
    "    if s.search(url):\n",
    "        if url in done.keys():\n",
    "            yield done[url]\n",
    "            pass\n",
    "        try:\n",
    "            # make a request to the url\n",
    "            r = requests.get(url, verify=False, timeout=10)\n",
    "        except:\n",
    "            # if the url does not return data, set to empty values\n",
    "            done[url] = \"Unable to reach website.\"\n",
    "            answer['author'] = None\n",
    "            answer['base'] = s.search(url).group()\n",
    "            answer['provider']=site\n",
    "            answer['published_date']=None\n",
    "            answer['text'] = \"Unable to reach website.\"\n",
    "            answer['title'] = None\n",
    "            answer['top_image'] = None\n",
    "            answer['url'] = url\n",
    "            answer['keywords']=None\n",
    "            answer['summary']=None\n",
    "            yield answer\n",
    "        # if url does not return successfully, set ot empty values\n",
    "        if r.status_code != 200:\n",
    "            done[url] = \"Unable to reach website.\"\n",
    "            answer['author'] = None\n",
    "            answer['base'] = s.search(url).group()\n",
    "            answer['provider']=site\n",
    "            answer['published_date']=None\n",
    "            answer['text'] = \"Unable to reach website.\"\n",
    "            answer['title'] = None\n",
    "            answer['top_image'] = None\n",
    "            answer['url'] = url\n",
    "            answer['keywords']=None\n",
    "            answer['summary']=None\n",
    "\n",
    "        # test if length of url content is greater than 500, if so, fill data\n",
    "        if len(r.content)>500:\n",
    "            # set article url\n",
    "            article = Article(url)\n",
    "            # test for python version because of html different parameters\n",
    "            if int(platform.python_version_tuple()[0])==3:\n",
    "                article.download(input_html=r.content)\n",
    "            elif int(platform.python_version_tuple()[0])==2:\n",
    "                article.download(html=r.content)\n",
    "            # parse the url\n",
    "            article.parse()\n",
    "            article.nlp()\n",
    "            # if parse doesn't pull text fill the rest of the data\n",
    "            if len(article.text) >= 200:\n",
    "                answer['author'] = \", \".join(article.authors)\n",
    "                answer['base'] = s.search(url).group()\n",
    "                answer['provider']=site\n",
    "                answer['published_date'] = article.publish_date\n",
    "                answer['keywords']=article.keywords\n",
    "                answer['summary']=article.summary\n",
    "                # convert the data to isoformat; exception for naive date\n",
    "                if isinstance(article.publish_date,datetime.datetime):\n",
    "                    try:\n",
    "                        answer['published_date']=article.publish_date.astimezone(pytz.utc).isoformat()\n",
    "                    except:\n",
    "                        answer['published_date']=article.publish_date.isoformat()\n",
    "                \n",
    "\n",
    "                answer['text'] = article.text\n",
    "                answer['title'] = article.title\n",
    "                answer['top_image'] = article.top_image\n",
    "                answer['url'] = url\n",
    "                \n",
    "                \n",
    "\n",
    "            # if previous didn't work, try another library\n",
    "            else:\n",
    "                doc = Paper(r.content)\n",
    "                data = doc.summary()\n",
    "                title = doc.title()\n",
    "                soup = BeautifulSoup(data, 'lxml')\n",
    "                newstext = \" \".join([l.text for l in soup.find_all(TAGS)])\n",
    "\n",
    "                # as we did above, pull text if it's greater than 200 length\n",
    "                if len(newstext) > 200:\n",
    "                    answer['author'] = None\n",
    "                    answer['base'] = s.search(url).group()\n",
    "                    answer['provider']=site\n",
    "                    answer['published_date']=None\n",
    "                    answer['text'] = newstext\n",
    "                    answer['title'] = title\n",
    "                    answer['top_image'] = None\n",
    "                    answer['url'] = url\n",
    "                    answer['keywords']=None\n",
    "                    answer['summary']=None\n",
    "                # if nothing works above, use beautiful soup\n",
    "                else:\n",
    "                    newstext = \" \".join([\n",
    "                        l.text\n",
    "                        for l in soup.find_all(\n",
    "                            'div', class_='field-item even')\n",
    "                    ])\n",
    "                    done[url] = newstext\n",
    "                    answer['author'] = None\n",
    "                    answer['base'] = s.search(url).group()\n",
    "                    answer['provider']=site\n",
    "                    answer['published_date']=None\n",
    "                    answer['text'] = newstext\n",
    "                    answer['title'] = title\n",
    "                    answer['top_image'] = None\n",
    "                    answer['url'] = url\n",
    "                    answer['keywords']=None\n",
    "                    answer['summary']=None\n",
    "        # if nothing works, fill with empty values\n",
    "        else:\n",
    "            answer['author'] = None\n",
    "            answer['base'] = s.search(url).group()\n",
    "            answer['provider']=site\n",
    "            answer['published_date']=None\n",
    "            answer['text'] = 'No text returned'\n",
    "            answer['title'] = None\n",
    "            answer['top_image'] = None\n",
    "            answer['url'] = url\n",
    "            answer['keywords']=None\n",
    "            answer['summary']=None\n",
    "            yield answer\n",
    "        yield answer\n",
    "\n",
    "    # the else clause to catch if invalid url passed in\n",
    "    else:\n",
    "        answer['author'] = None\n",
    "        answer['base'] = s.search(url).group()\n",
    "        answer['provider']=site\n",
    "        answer['published_date']=None\n",
    "        answer['text'] = 'This is not a proper url'\n",
    "        answer['title'] = None\n",
    "        answer['top_image'] = None\n",
    "        answer['url'] = url\n",
    "        answer['keywords']=None\n",
    "        answer['summary']=None\n",
    "        yield answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6f1f0066",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/juan/miniconda3/envs/nlp/lib/python3.7/site-packages/urllib3/connectionpool.py:1020: InsecureRequestWarning: Unverified HTTPS request is being made to host 'www.snam.it'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
      "  InsecureRequestWarning,\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'author': '',\n",
       "  'base': 'https://www.snam.it',\n",
       "  'provider': 'snam',\n",
       "  'published_date': None,\n",
       "  'keywords': ['società',\n",
       "   'sostenibile',\n",
       "   'page',\n",
       "   'è',\n",
       "   'tecnologicamente',\n",
       "   'una',\n",
       "   'la',\n",
       "   'sicurezza',\n",
       "   'transizione',\n",
       "   'snam',\n",
       "   'delle'],\n",
       "  'summary': 'Snam è una delle principali società di infrastrutture energetiche al mondo e una delle maggiori aziende quotate italiane per capitalizzazione.\\nGrazie a una rete sostenibile e tecnologicamente avanzata, garantisce la sicurezza degli approvvigionamenti e abilita la transizione energetica.',\n",
       "  'text': 'Snam è una delle principali società di infrastrutture energetiche al mondo e una delle maggiori aziende quotate italiane per capitalizzazione. Grazie a una rete sostenibile e tecnologicamente avanzata, garantisce la sicurezza degli approvvigionamenti e abilita la transizione energetica. >>>',\n",
       "  'title': 'Snam home page',\n",
       "  'top_image': 'https://www.snam.it/system/modules/com.ntt.snam.responsive/resources/assets/favicon/apple-touch-icon.png',\n",
       "  'url': 'https://www.snam.it/it/index.html'}]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = \"https://es.wikipedia.org/wiki/Energ%C3%ADa\"\n",
    "url = \"https://www.expansion.com/empresas/energia.html\"\n",
    "url = \"http://www.aemet.es/es/portada\"\n",
    "url = \"https://www.endesa.com\"\n",
    "url = \"https://www.meteologica.com\"\n",
    "url = \"https://www.ree.es/es\"\n",
    "url = \"https://www.eia.gov\"\n",
    "url = \"https://www.thomsonreuters.com/en/products-services/energy/top-100.html\"\n",
    "url = \"http://www.enagas.es\"\n",
    "url = \"https://www.snam.it/it/index.html\"\n",
    "results = get_url_text(url)\n",
    "list(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b77a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a481c2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "nlp"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
